# Backend Dockerfile for GPU Server
# Includes Python, CUDA, PyTorch, and all dependencies for YOLO training/inference

# Use NVIDIA CUDA base image with Python
FROM nvidia/cuda:12.1-cudnn8-devel-ubuntu22.04

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3.11 \
    python3.11-venv \
    python3.11-dev \
    python3-pip \
    libgl1-mesa-glx \
    libglib2.0-0 \
    libsm6 \
    libxext6 \
    libxrender-dev \
    libgomp1 \
    wget \
    curl \
    git \
    && rm -rf /var/lib/apt/lists/*

# Create and activate virtual environment
RUN python3.11 -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Upgrade pip
RUN pip install --upgrade pip wheel setuptools

# Install PyTorch with CUDA support
RUN pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Install TensorRT (for optimized inference)
RUN pip install tensorrt nvidia-tensorrt --extra-index-url https://pypi.nvidia.com

# Install YOLO and other ML dependencies
RUN pip install \
    ultralytics>=8.0.0 \
    opencv-python-headless>=4.8.0 \
    numpy>=1.24.0 \
    pillow>=10.0.0 \
    cupy-cuda12x>=12.0.0

# Install FastAPI server dependencies
RUN pip install \
    fastapi>=0.100.0 \
    uvicorn[standard]>=0.23.0 \
    sse-starlette>=1.6.0 \
    python-multipart>=0.0.6 \
    pydantic>=2.0.0 \
    psutil>=5.9.0

# Create app directory
WORKDIR /app

# Copy Python backend code
COPY electron/python/ /app/

# Create directories for models and projects
RUN mkdir -p /app/models/keypoint /app/models/detection /root/.follicle-labeller/projects

# Expose port
EXPOSE 5555

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:5555/health || exit 1

# Run the server
CMD ["python", "blob_server.py", "--host", "0.0.0.0", "--port", "5555"]
